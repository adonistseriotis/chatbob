# -*- coding: utf-8 -*-
"""giota5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nqcbokHOUJ5Rbk4kU7lMDE6v9BsBiohe
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors
import nltk
from nltk.corpus import stopwords, names
import string
import joblib
import scipy as sp
import textwrap
import csv
import random

import tensorflow as tf
from tensorflow.keras import datasets, layers, models, Input
import datetime
import matplotlib.pyplot as plt
from sklearn.metrics import  classification_report

"""## Εισαγωγή του Dataset"""

dataset_url = "https://raw.githubusercontent.com/JULIELab/EmoBank/master/corpus/emobank.csv"
df_data = pd.read_csv(dataset_url,quoting = 0)

print(df_data)

print(df_data.describe())

corpus =  df_data.iloc[:, [5]]
vad = df_data.iloc[:, [2,3,4]]

"""## Dataset Ερωτήσεων

"""

questions = ['Describe your perfect day.',
             'Discuss 5 things you wish others knew about you.',
             'Write one paragraph about what made you happy today.',
             'Identify three short term goals and one long term goal.',
             'What is a self-care strategy you have always been curious in trying?',
             'What inspires you?',
             'Make a list of 15 things that make you happy.',
             'Write about an important person in your life who you are grateful for. What do you admire about this person, why have they had such an impact on your life, and what would you like to tell them?',
             'What are your best character traits?',
             'Write down three to five things that trigger feelings of anxiety in you and identify one to three coping strategies you can attempt to implement in response to these.',
             "Pick one positive word you'd like to focus on today and describe what it makes you feel or what you associate with it.",
             'Describe a situation in which you helped someone else. How do you think it made them feel? How did it make you feel?',
             'What self-care strategies have you used in the past? Rate them from 1-10, 10 being the most effective and helpful, 1 being the least. ',
             'What advice would you give to someone going through a hard time?',
             'What is a quote that you want to manifest on for the rest of the week? How can you apply it to your life every day this week?',
             'Describe a hardship you have had. How does it make you feel reflecting back on it? What do you feel you learned?',
             'What makes you feel fulfilled? ',
             'Write a letter to someone who has done you wrong. Discuss what they did and how it made you feel. Try to forgive them and rip it up if it feels right.',
             'What things are you avoiding dealing with?',
             'What do you need right now?'
             ]

"""## TF-IDF Vectorizer

Αρχικά εξετάζουμε το corpus που προκύπτει χωρίς προεπεξεργασία και βελτιστοποίηση. Παρατηρούμε ότι υπάρχουν πολλές λέξεις που δεν προσφέρουν κάποια χρήσιμη πληροφορία ενώ μεγάλος είναι και ο αριθμός συγγενικών λέξεων,
"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
tf_id_array = vectorizer.fit_transform(corpus['text'].tolist()).toarray()
print(tf_id_array.shape)
print(vectorizer.get_feature_names())

"""Θα χρησιμοποιήσουμε stopwords για να μειώσουμε την διάσταση του vector που προκύπτει διατηρώντας όμως το νοηματικό περιεχόμενο των περιγραφών."""

import nltk
nltk.download('stopwords')
nltk.download('names')

from nltk.corpus import stopwords, names
import string
import re
my_stopwords = stopwords.words("english") + names.words()
my_stopwords += ["''", "'d", "'ll", "'re", "'ve", '--', '...', '1', '10', '2', '20', '3', '4', '``', '–', '—', '’', '“', '”']
my_stopwords = set(my_stopwords)

print(my_stopwords)

def thorough_filter(words):
    filtered_words = []
    regexp = re.compile('[0-9]+')
    for word in words:
      if not regexp.search(word):
        pun = []
        for letter in word:
          pun.append(letter in string.punctuation.strip('!').strip('?'))
        if not all(pun):
          filtered_words.append(word)
    return filtered_words

filtered = thorough_filter(vectorizer.get_feature_names())
print(filtered)
print(len(filtered))

"""Μετά από δοκιμή και βελτιστοποίηση διαφόρων stemmers και lemmatizers, είδαμε ότι ο SnowballStemmer() δίνει τις καλύτερες προτάσεις."""

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('rslp')

from nltk.stem.snowball import SnowballStemmer
class SnowballTokenizer(object):
    def __call__(self, text):
        words = [word for word in nltk.word_tokenize(text) if (word not in my_stopwords)]
        words = thorough_filter(words)
        return [SnowballStemmer("english").stem(word) for word in words]

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(tokenizer=SnowballTokenizer(),  max_df=0.95, min_df=3, sublinear_tf=True)
tf_id_array = vectorizer.fit_transform(corpus['text'].tolist()).toarray()
print(tf_id_array.shape)
print(vectorizer.get_feature_names())

print(tf_id_array.shape)

print(vad.shape)

"""## Training and Test Sets"""

splits = np.array(df_data.iloc[:,1].tolist())
train_set = tf_id_array[splits=='train']
train_labels = vad[splits=='train'].to_numpy()
test_set = tf_id_array[splits=='test']
test_labels = vad[splits=='test'].to_numpy()
dev_set = tf_id_array[splits=='dev']
dev_labels = vad[splits=='dev'].to_numpy()

sample_length = train_set.shape[1]

print(train_set.shape)
print(train_labels.shape)

train_labels.shape

"""## VAD to Basic Emotion Conversion"""

emotion_coeff = {
  'anger': np.array([-0.43, 0.67, 0.34]),
  'joy': np.array([0.76, 0.48, 0.35]),
  'surpise': np.array([0.4, 0.67, -0.13]),
  #'disgust': np.array([-0.6, 0.35, 0.11]),
  'fear': np.array([-0.64, 0.6, -0.43]),
  'sadness': np.array([-0.63, 0.27, -0.33])
}

vad2emotion = {
    0: 'anger',
    1: 'joy',
    2: 'surprise',
    #3: 'disgust',
    3: 'fear',
    4: 'sadness'
}

for k,v in emotion_coeff.items():
  emotion_coeff[k] = 2*v+3

emo_coeff = np.array(list(emotion_coeff.values()))

import pprint 
pp = pprint.PrettyPrinter(indent=4)
pp.pprint(emotion_coeff)

neigh = NearestNeighbors(n_neighbors=1)
neigh.fit(emo_coeff)

train_labels_emo_dst, train_labels_emo = neigh.kneighbors(train_labels)
test_labels_emo_dst, test_labels_emo = neigh.kneighbors(test_labels)

"""## Helper Functions"""

# plot diagnostic learning curves
def summarize_diagnostics(history):
	plt.figure(figsize=(8, 8))
	plt.suptitle('Training Curves')
	# plot loss
	plt.subplot(211)
	plt.title('Loss')
	plt.plot(history.history['loss'], color='blue', label='train')
	plt.plot(history.history['val_loss'], color='orange', label='val')
	plt.legend(loc='upper right')
	# plot accuracy
	plt.subplot(212)
	plt.title('Accuracy')
	plt.plot(history.history['accuracy'], color='blue', label='train')
	plt.plot(history.history['val_accuracy'], color='orange', label='val')
	plt.legend(loc='lower right')
	return plt
 
# print test set evaluation metrics
def model_evaluation(model, evaluation_steps, test_set, test_labels):
  print('\nTest set evaluation metrics')
  print(model.evaluate(test_set, steps = evaluation_steps))

def model_report(model, history, test_set, test_labels, evaluation_steps = 10):
  plt = summarize_diagnostics(history)
  plt.show()
  model_evaluation(model, evaluation_steps, test_set, test_labels)

def train_model(model, train_set, train_labels, val_set, val_labels, epochs = 10, steps_per_epoch = 2, validation_steps = 1, tensorboard_entry = False, early_stopping = False, learning_rate = False):
                
  log_dir = "./logs/fit/" + model.name + "/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

  callbacks = []
  if tensorboard_entry:
    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1))
  if early_stopping:
    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta = 0 , patience = 40, restore_best_weights= True))
  if learning_rate:
      callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=10, min_lr=0.00001))
  
  history = model.fit(train_set, train_labels, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=(val_set, val_labels), validation_steps=validation_steps, callbacks = callbacks)
  return(history)

"""## Model"""

load = True

def init_zuccbot():

  model = models.Sequential(name="zuccbot")
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(128, activation='relu'))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(32, activation='relu'))
  model.add(layers.Dense(16, activation='relu'))
  model.add(layers.Dense(3))
  
  model.compile(optimizer="rmsprop", loss='mean_squared_error', metrics=['accuracy'])

  return model

if not load:
  model = init_zuccbot()
  history = train_model(model, train_set, train_labels, dev_set, dev_labels, 300, 30, 5)
  model.save("chatbob-model")

model_report(model, history, test_set, test_labels)

if load:
  reconstructed_model = models.load_model("chatbob-model")

vad_preds = model.predict(train_set)
emo_preds = neigh.kneighbors(vad_preds)
report = classification_report(train_labels_emo, emo_preds[1], zero_division=0)
print(report)

vad_preds = model.predict(test_set)
emo_preds = neigh.kneighbors(vad_preds)
report = classification_report(test_labels_emo, emo_preds[1], zero_division=0)
print(report)

"""# Emotion Detection"""

X = [[2.74, 3.25, 3.0], [2.2, 4.25, 3.0], [3, 3.5, 4.0]]
neigh.kneighbors(X)

def analyseSentence(sentence):
  inp = vectorizer.transform([sentence]).toarray()
  vad_pred = model.predict(inp)
  emo_dst, emo_pred = neigh.kneighbors(vad_pred, 3)
  emos = [vad2emotion[x] for x in emo_pred.ravel()]
  return 1/emo_dst.ravel(), emos

analyseSentence('! ?')

def analyseQuestionnaire(answers):
  emotions = ['anger', 'joy', 'surprise', 'fear', 'sadness']
  emo_vals = { x:0 for x in emotions }
  for sen in answers:
    emo_ints, emos = analyseSentence(sen)
    for emo, emo_int in zip(emos, emo_ints):
      emo_vals[emo] += emo_int
  emo_vals = { emo: emo_int/sum(emo_vals.values()) for emo, emo_int in emo_vals.items()}
  return emo_vals

answers_sad = ['i woke up today and ate breakfast', "I told her, whenever I'm sad, my grandmother gives me karate chops.", "I thought it was very sad that some Members walked out of this House when President Klaus was speaking, and that that should happen during the current Czech Presidency.", "Then, God will rule mankind and restore the human race to the life of happiness and peace he originally intended."]

answers_scary = ["Scary thing is, he tracked me down by monitoring my signal strength.", "Before moving from there recently to a safer place outside the city, the headquarters family had some scary experiences.", "I can't do this crazy scary short chick screaming at me on the street.",'i woke up today and ate breakfast' ]

answers_happy = ["When he came back to Rosewood... the things he was saying, I was sure that he was in love with you.", "Isn't the sea lovely, Mummy?", "Did you not think that we would not search the world to get justice for those good people that we loved?", "ella considers Jacob only as a friend, but despite her engagement to Edward, she shares a kiss with him, and realizes that she loves him, too, but loves Edward more."]

emo_ints = analyseQuestionnaire(answers_scary)
print(emo_ints)

plt.bar(emo_ints.keys(), emo_ints.values())
plt.title("Emotion Intensities")
plt.ylabel("Intensity")
plt.show()

analyseQuestionnaire(questions)

answers = []
while(1):
  #index = random.sample(range(20),1)
  ans = input(questions[random.randrange(20)])
  if ans == 'exit':
    break
  answers.append(ans)
analyseQuestionnaire(answers)